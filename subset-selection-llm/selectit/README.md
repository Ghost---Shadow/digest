# SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection

## Meta

* **Name**: SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection
* **Journal**: Neural Information Processing Systems (NeurIPS)
* **Year**: 2024
* **Author**: 1Institute of Computing and Intelligence, Harbin Institute of Technology, Shenzhen, China; 2NLP2CT Lab, Department of Computer and Information Science, University of Macau
* **Code**: [Github link](https://github.com/Blue-Raincoat/SelectIT)
* **One liner**: SelectIT uses the intrinsic uncertainty of large language models to select high-quality instruction tuning data without the need for additional resources.
* **Model**: LLaMA-2 (7B, 13B, 70B)
* **Datasets**: Alpaca-GPT4, Selective Alpaca
* **Baselines**: Alpaca-GPT4, LIMA, AlpaGasus, Q2Q, Instruction Mining

## Formulas

Below is a detailed breakdown of each equation and its respective variables using MathJax-style LaTeX.

1. **Equation (1): Quality score from LLM**

   $$\text{Quality} \propto \text{Score} \in [1, K] = M (RP, S)$$

   **Explanation of Variables:**

   - $M$: Represents the foundational language model (LLM) used to assess the quality of the instruction tuning (IT) data.
   - $RP$: Denotes the rating prompt, a specific prompt designed to elicit a qualitative score from the model regarding the IT sample.
   - $S$: Stands for the sample being evaluated, which contains both the input $X$ and the response $Y$.
   - $\text{Score} \in [1, K]$: Is the quality score assigned by the model. The score lies in the discrete range from 1 to $K$, where $K$ indicates the maximum score.
   - $\text{Quality} \propto \text{Score}$: Indicates that the overall quality of the IT data is proportional to the score generated by the model $M$ when given $RP$ and $S$.

2. **Equation (2): Token-level self-reflection**

   $$S_{base} = \arg\max_{k \in \{1,\dots,K\}} k, \quad P'_{k} = \frac{P_{k}}{\sum_{j=1}^{K}P_{j}}$$

   **Explanation of Variables:**

   - $S_{base}$: The base token-level score determined by selecting the token $k$ that maximizes the probability. It is the token index with the highest probability.
   - $k \in \{1, \dots, K\}$: Represents the indices of the tokens being considered. $K$ is the total number of tokens (or scoring categories) provided by the model.
   - $P_{k}$: The (raw) probability assigned to token $k$ by the model.
   - $P'_{k}$: The normalized probability of token $k$, computed by dividing $P_{k}$ by the sum of probabilities over all tokens $\sum_{j=1}^{K} P_{j}$. This normalization ensures that $\sum_{k=1}^{K} P'_{k} = 1$.

3. **Equation (3): Token-level score**

   $$S_{\text{token}} = S_{base} \times \frac{1}{K - 1} \sum_{i=1}^{K} |P'_{i} - P'_{S_{base}} |$$

   **Explanation of Variables:**

   - $S_{\text{token}}$: The computed token-level score that reflects the internal uncertainty among the token probabilities.
   - $S_{base}$: The baseline token score from Equation (2), representing the index of the token with the highest probability.
   - $K$: The total number of tokens (or rating categories) considered.
   - $P'_{i}$: The normalized probability for token $i$ (for $i = 1, 2, \dots, K$).
   - $P'_{S_{base}}$: The normalized probability corresponding to the token that achieved $S_{base}$.
   - The term $\frac{1}{K - 1} \sum_{i=1}^{K} |P'_{i} - P'_{S_{base}} |$ calculates the average absolute deviation between each token's normalized probability and that of the highest-probability token. Multiplying by $S_{base}$ incorporates the confidence (or value) of the top token with the overall uncertainty represented by the deviation.

4. **Equation (4): Sentence-level self-reflection**

   $$S_{\text{sent}} = \frac{\text{Avg}\{S_{\text{token}, i}\}_{i=1}^{K}}{1 + \alpha \times \text{Std}\{S_{\text{token}, i}\}_{i=1}^{K}}$$

   **Explanation of Variables:**

   - $S_{\text{sent}}$: The sentence-level score that aggregates token-level scores across multiple prompts.
   - $S_{\text{token}, i}$: The token-level score for the $i$-th rating prompt.
   - $\text{Avg}\{S_{\text{token}, i}\}_{i=1}^{K}$: The average of the token-level scores across all $K$ rating prompts. This term captures the overall central tendency of the token-level evaluations.
   - $\text{Std}\{S_{\text{token}, i}\}_{i=1}^{K}$: The standard deviation of the token-level scores, quantifying the spread or variability of the scores across the prompts.
   - $\alpha$: A hyperparameter that scales the effect of the standard deviation. It adjusts the influence of variability (uncertainty) on the final sentence-level score.
   - The denominator $1 + \alpha \times \text{Std}\{S_{\text{token}, i}\}_{i=1}^{K}$ effectively penalizes high variability among token-level scores, thereby favoring samples where the token-level assessments are consistent.

5. **Equation (5): Model-level self-reflection**

   $$\text{Quality} \propto S_{\text{model}} = \sum_{i=1}^{N} \left(\frac{\theta_{i}}{\sum_{j=1}^{N} \theta_{j}}\right) \times S_{\text{sent}, i}$$

   **Explanation of Variables:**

   - $S_{\text{model}}$: The model-level score that provides a comprehensive evaluation of the IT data by incorporating scores from multiple foundation models.
   - $N$: The total number of foundation models used for evaluation.
   - $\theta_{i}$: The parameter count (or a representative measure of model capacity) of the $i$-th foundation model. This serves as a weight, giving more influence to models with larger capacities.
   - $S_{\text{sent}, i}$: The sentence-level score as computed by the $i$-th model (from Equation (4)).
   - $\frac{\theta_{i}}{\sum_{j=1}^{N} \theta_{j}}$: The normalized weight for the $i$-th model, ensuring that the summation of all model weights equals 1.
   - $\text{Quality} \propto S_{\text{model}}$: Indicates that the overall quality of the IT data is proportional to the aggregated (weighted) sentence-level scores across all models.

### Summary

In summary, the SelectIT method assesses the quality of IT data by progressively incorporating different levels of self-reflection:

- **Token Level:** Evaluates individual token probabilities and their uncertainties (Equations 2 and 3).
- **Sentence Level:** Aggregates these token-level evaluations across multiple prompts, adjusting for consistency (Equation 4).
- **Model Level:** Combines evaluations from multiple foundation models in a weighted manner based on their capacities (Equation 5).

This multi-granular approach allows for a nuanced assessment, balancing the precision of token-level judgments with the broader perspective of model-level uncertainty in the data selection process.

## Training Flow

### Training Flow

1. **Define the Evaluation Process**:
   - Utilize the foundation language model (LLM) to determine rating scores for IT data samples based on internal uncertainty in the LLM.
   
2. **Implement Token-level Self-Reflection**:
   - For a given sample $S = (X, Y)$ and a rating prompt $RP$, calculate the next-token probability $P'_{k}$ via the foundation model for tokens $k \in \{1, ..., K\}$.
   - Determine the token-level score $S_{token}$ by adjusting $S_{base}$, the token with the highest normalized probability: 
   $$
   S_{token} = S_{base} \cdot \frac{1}{K - 1} \sum_{i=1}^K |P'_i - P'_{S_{base}}|
   $$
   
3. **Implement Sentence-level Self-Reflection**:
   - Define $K$ semantically similar rating prompts \{RP_0, RP_1, ..., RP_K\}.
   - Compute sentence-level score $S_{sent}$ by averaging token-level scores from these prompts and integrating the uncertainty:
   $$
   S_{sent} = \frac{\text{Avg}\{S_{token}\}}{1 + \alpha \cdot \text{Std}\{S_{token}\}}
   $$
   
4. **Implement Model-level Self-Reflection**:
   - For multiple foundation models with parameter counts $\{\theta_1, \theta_2, ..., \theta_N\}$, compute a comprehensive model-level score $S_{model}$ by weighing sentence-level scores:
   $$
   S_{model} = \sum_{i=1}^N \left( \frac{\theta_i}{\sum_{j=1}^N \theta_j} \cdot S_{sent}^{i} \right)
   $$

5. **Data Selection**:
   - Use $S_{model}$ to rank the dataset samples.
   - Select top samples as high-quality IT data.

6. **Apply SelectIT on Dataset**:
   - Use SelectIT to evaluate and curate the Selective Alpaca dataset, resulting in a superior IT dataset for model training.

### Training Flow Code

```python
def select_it_data(samples, foundation_models, rating_prompts, alpha):
    def token_reflection(sample, rating_prompt):
        # Compute next-token probabilities
        probs = model.compute_probabilities(sample, rating_prompt)
        S_base = max(prob.index for prob in probs)
        # Token-level reflection
        token_score = S_base * (1 / (len(probs) - 1)) * sum([abs(prob - probs[S_base]) for prob in probs])
        return token_score
    
    def sentence_reflection(sample):
        scores = [token_reflection(sample, rp) for rp in rating_prompts]
        avg_score = mean(scores)
        uncertainty = std(scores)
        return avg_score / (1 + alpha * uncertainty)
    
    def model_reflection(sample):
        sent_scores = [sentence_reflection(sample, model) for model in foundation_models]
        weighted_score = sum([(model.param_count / total_params) * score for model, score in zip(foundation_models, sent_scores)])
        return weighted_score
    
    # Rank samples
    ranked_samples = sorted(samples, key=lambda x: model_reflection(x), reverse=True)
    return ranked_samples[:int(len(samples) * QUALITY_THRESHOLD)]

# Example usage
selected_data = select_it_data(dataset_samples, foundation_models=llama_models, rating_prompts=rating_prompts, alpha=0.2) 
```

This pseudocode outlines the high-level process of applying SelectIT in Python using PyTorch to evaluate the IT data quality and select a high-quality dataset for training LLMs.

## Inference Flow

### Inference Flow

1. **Load the Foundation Language Model ($M$)**.
2. **Prepare Instruction Tuning (IT) Dataset $D$**, consisting of samples $S = (input \, X, response \, Y)$.
3. **For Each Sample $S$ in the IT Dataset**:
    1. Use token-level uncertainty: For each token $k \in \{1, \ldots, K\}$, calculate the probability $P_k$ of the next token. Compute the base score $S_{base}$ using $S_{base} = \arg \max_k P'_k$, where $P'_k = \frac{P_k}{\sum_{j=1}^K P_j}$.
    2. Token-level self-reflection: Enhance $S_{base}$ to $S_{token}$ by accounting for the average disparity in token-score probabilities using: 
       $$
       S_{token} = S_{base} \times \frac{1}{K-1} \sum_{i=1}^{K} |P'_i - P'_{S_{base}}|
       $$
    3. Sentence-level uncertainty: Use $K$ semantically similar rating prompts \{RP0, ..., RPK\} to generate a series of quality scores \{$S_{token}^0, ..., S_{token}^K$\}. Compute sentence score $S_{sent}$ using:
       $$
       S_{sent} = \frac{\text{Avg}\{S_{token}^i\}_{i=1}^K}{1 + \alpha \times \text{Std}\{S_{token}^i\}_{i=1}^K}
       $$
    4. Model-level self-reflection: Aggregate sentence scores across multiple foundation models $\{M_1, M_2, ..., M_N\}$ based on their parameter counts $\{\theta_1, ..., \theta_N\}$, computing:
       $$
       S_{model} = \left( \sum_{i=1}^{N} \frac{\theta_i}{\sum_{j=1}^{N} \theta_j} \right) \times S_{sent}^i
       $$
4. **Generate Final Sample Quality Score $S_{model}$ and Sort All Samples in $D$ by $S_{model}$.
5. **Select the Top Proportion of Samples with the Highest $S_{model}$ Scores as High-quality IT Data.

### Inference Flow Code

```python
def compute_token_score_probabilities(model, input_data, rating_prompts, K):
    # Compute next-token probabilities for each prompt
    token_scores = []
    for prompt in rating_prompts:
        logits = model(input_data + prompt)
        probabilities = F.softmax(logits, dim=-1)
        token_scores.append(probabilities[:, :K].mean(dim=0))
    return torch.stack(token_scores)

def compute_token_reflection_score(token_scores, K):
    disparities = torch.abs(token_scores - token_scores.max(dim=1, keepdim=True)[0])
    token_reflection = token_scores.max(dim=1)[0] * (disparities.sum(dim=1) / (K - 1))
    return token_reflection

def compute_sentence_reflection_score(token_reflection_scores, alpha):
    avg_score = token_reflection_scores.mean()
    std_score = token_reflection_scores.std()
    sentence_reflection = avg_score / (1 + alpha * std_score)
    return sentence_reflection

def compute_model_reflection_score(models, sentence_reflection_scores, model_param_counts):
    weights = torch.tensor(model_param_counts) / sum(model_param_counts)
    model_reflection = (weights * sentence_reflection_scores).sum()
    return model_reflection

def select_high_quality_data(models, data, rating_prompts, alpha, K, top_percentage):
    all_model_scores = []
    for sample in data:
        token_scores = compute_token_score_probabilities(models[0], sample['input'], rating_prompts, K)
        token_reflection_scores = compute_token_reflection_score(token_scores, K)
        sentence_reflection = compute_sentence_reflection_score(token_reflection_scores, alpha)
        
        # Repeat with different models if necessary
        model_scores = []
        for model in models:
            model_score = compute_model_reflection_score(model, sentence_reflection, model.param_count)
            model_scores.append(model_score)
        all_model_scores.append(sum(model_scores) / len(model_scores))  # Average over models
    
    # Select top samples based on model reflection scores
    sorted_indices = sorted(range(len(all_model_scores)), key=lambda i: all_model_scores[i], reverse=True)
    selected_samples = [data[i] for i in sorted_indices[:int(len(data) * top_percentage)]]
    return selected_samples
```

This pseudocode describes the inferencing process of SelectIT, illustrating how it ranks IT data samples based on their estimated quality and how high-quality samples are selected using uncertainty from multiple LLM levels.

## Experiments

### List of Experiments

1. **Annotation Budget Ablations**: Investigating the impact of varying annotation budgets on the performance of Selective Alpaca (Figures and tables discussed in text).

2. **Efficiency of SelectIT**: A comprehensive comparison of time cost and efficiency between SelectIT and other data selection methods, such as ChatGPT and GPT-4 API (Table 9).

3. **Performance Across Scales**: Evaluation of the robustness of SelectIT when applied to various scales of foundation models like LLaMA-2 and LLaMA-3 (Table 6).

4. **Domain-Specific Task Evaluation**: Applying SelectIT to improve machine translation capabilities within domain-specific tasks, examining its effectiveness in enhancing the ALMA model (Table 8).

5. **Different Reflection Strategy Effects**: Analysis of the distinct roles played by token, sentence, and model-level self-reflection in the data selection process of SelectIT (Table 4).

6. **Data Representation Analysis**: Examination of data representation and characteristics comparing Selective Alpaca with original datasets to understand the distribution of selected data points (Figures 4 and 5).

7. **Correlation Study of Sample Characteristics**: Evaluation of the relationship between sample computation complexity, average length, and model performance driving insights into high-quality IT data traits (Figure 6).

8. **Robustness to Instruction Tuning Datasets**: Testing the adaptability of SelectIT by deploying on various widely-utilized instruction tuning datasets like WizardLM and Orca-GPT4 (Table 7).

9. **Statistical Significance Testing**: Application of statistical significance tests to ensure the reliability of performance improvements with SelectIT (discussed in Appendix A.1).

10. **MT LLMs Impact Study**: Impact analysis of SelectIT on Machine Translation large language models, evaluating multilingual translation effectiveness (Table 11).

## Proofs

### List of Proofs

The paper "SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection" does not explicitly state proofs for mathematical theorems or propositions. However, it discusses the processes and methodologies underlying its approach. As such, the paper does not contain a list of proofs in the traditional mathematical sense.